## 硬件优化
### CPU配置
- CPU繁忙的原因有以下几个
  - 线程中有无限空循环、无阻塞、正则匹配或者单纯的计算
  - 发生了频繁的GC
  - 多线程的上下文切换
- CPU选择
  - 大多数ES部署往往对CPU要求不高
  - 因此，相对其它资源，具体配置多少个（CPU）不是那么关键
  - 你应该选择具有多个内核的现代处理器，常见的集群使用2到8个核的机器
  - 如果你要在更快的CPUs和更多的核数之间选择，选择更多的核数更好
  - 多个内核提供的额外并发远胜过稍微快一点点的时钟频率
## 内存配置
- 如果有一种资源是最先被耗尽的，它可能是内存
- 排序和聚合都很耗内存，所以有足够的堆空间来应付它们是很重要的
- 即使堆空间是比较小的时候，也能为操作系统文件缓存提供额外的内存
- 64GB内存的机器是非常理想的，但是32GB和16GB机器也是很常见的
- 由于ES构建基于Lucene，而Lucene设计强大之处在于Lucene能够很好的利用操作系统内存来缓存索引数据，以提供快速的查询性能
- Lucene的索引文件segements是存储在单文件中的，并且不可变
- 对于OS来说，能够很友好地将索引文件保持在cache中，以便快速访问
- 因此，我们很有必要将一半的物理内存留给Lucene
- 另一半的物理内存留给ES（JVM heap）
- 静止swap
  - 禁止swap，一旦允许内存与磁盘的交换，会引起致命的性能问题
  - 可以通过elasticsearch.yml中bootstrap.memory_lock=true来禁止swap,以保持JVM锁定内存，保证ES的性能
### 磁盘
- 磁盘对所有的集群都很重要，对大量写入的集群更是加倍重要(例如那些存储日志数据的)
- 硬盘是服务器上最慢的子系统，这意味着那些写入量很大的集群很容易让硬盘饱和，使得它成为集群的瓶颈
- 固态硬盘相比于任何旋转介质(机械硬盘，磁带等),无论随机写还是顺序写，都会对IO有较大的提升
## 索引优化设置
### 批量提交
- 当有大量数据提交的时候，建议采用批量提交(Bulk操作)
- 此外使用bulk请求时，每个请求不超过几十M,因为太大会导致内存使用过大
### 增加Refresh时间间隔
- 为了提高索引性能，ES在写入数据的时候，采用延迟写入的策略
- 即数据先写到内存中，当超过默认1秒回进行一次写入操作，就是将内存中segment数据刷新到磁盘中
- 此时我们才能将数据搜索出来，所以这就是为什么ES提供的最近实时搜索功能，而不是实时搜功能
- 如果我们的系统对数据延迟要求不高的话，我们可以通过延迟refresh时间间隔
- 可以有效地减少segment合并压力，提高索引速度
  - 比如在做全链路跟踪的过程中，我们就将index.refresh_interval设置为30s,减少refresh次数
  - 在进行全量索引时，可以将refresh次数临时关闭，数据导入成功后再打开到正常模式，比如30s
### index_buffer_size
- 索引缓冲的设置可以控制多少内存分配给索引进程
- 这是一个全局配置，会应用与一个节点上所有不同的分片上
~~~ sh
indices.memory.index_buffer_size: 10%
indices.memory.index_buffer_size: 48mb
~~~
- indices.memory.index_buffer_size接受一个百分比或者一个表示字节大小的值
- 默认是10%，意味着分配给节点的总内存的10%用来做索引缓冲的大小
- 这个数值被分到不同的分片(shards)上
- 如果设置的是百分比，还可以设置min_index_buffer_size（默认48MB）和max_index_buffer_size(默认没有上限)
### 修改translog相关的设置
- 一是控制数据从内存到硬盘的操作频率，以减少硬盘IO
- 可将sync_interval的时间设置大一些（默认为5s）
~~~ sh
index.translog.sync_interval:5s
~~~
- 也可以控制tranlog数据块的大小，达到threshold大小时，才会flush到Lucene索引文件(默认为512m)
~~~ sh
index.translog.flush_threshold_size:512mb
~~~
### 减少副本数量
- ES默认副本数量为3个，虽然这样会提高集群的可用性，增加搜索的并发数，但是同事也会影响写入索引的效率
- 在索引过程中，需要把更新的文档发到副本节点上，等副本节点生效后再进行返回结束
- 使用ES做业务搜索的时候，建议副本数目还是设置为3个
- 但是像内部ELK日志系统，分布式跟踪系统中，完全可以将副本数目设置为1个
### 其他
- 注意_id字段的使用
  - 应尽可能避免自定义_id，以避免针对ID的版本管理
  - 建议使用ES的默认ID生成策略或使用数字类型ID做为主键
- 注意_all字段及_source字段的使用
  - _all字段包含了所有的索引字段，方便做全文检索，如果无此需求，可以禁用
  - _source存储了原始的document内容，如果没有获取原始文档数据的需求
    - 可通过设置includes、excludes属性来定义放入_source的字段
- 合理的配置使用index属性
  - 合理的配置使用index属性，analyzed和not_analyzed,根据业务需求来控制字段是否分词或不分词
  - 只有groupby需求的字段，配置时就设置成not_analyzed，以提高查询或聚类的效率
## 查询方面优化
### 路由优化
- 当我们查询文档的时候，ES如何知道一个文档应该存放到那个分片中呢
- 它其实是通过下面这个公式来计算出来的
~~~ sh
shard - hash(routing) % number_of_primary_shards
~~~
- routing默认值是文档的id，也可以采用自定义值，比如用户ID
- 不带routing查询
  - 在查询的时候因为不知道要查询的数据具体在哪个分片上，所以整个过程分为2个步骤
  - 分发：请求到达协调节点后，协调节点将查询请求分发到每个分片上
  - 聚合：协调节点搜索到每个分片上查询结果，再将查询的结果进行排序，之后给用户返回结果
- 带routing查询
  - 查询的时候，可以直接根据routing信息定位到某个分配查询，不需要查询所有的分配，经过协调节点排序
  - 向上面自定义的用户查询，如果routing设置为userid的话，就可以直接查询出数据来，效率提升很多
### Filter VS Query
> 尽可能使用过滤器上下文(Filter)替代查询上下文（Query）
- Query:此文档与查询子句的匹配程度如何？
  - Query需要计算相关性分数
- Filter：此文档和查询子句匹配吗？
  - ES针对Filter查询只需要回答[是]或者[否],同时Filter结果可以缓存
### 深度翻页
- ES的分页是基于Lucene的跳过扫描（skip/seek）+排序缓存机制
- 当from越大时，ES仍然要扫描前from+size条记录，然后再丢弃前面的结果
- 会带来：
  - 内存占用徒增
  - 查询延迟变高
  - 甚至导致JVM full GC/OOM
- 建议：最后一条记录的排序字段值
~~~ json
GET /user/_search
{
  "query": {
    "term": { "status": 1 }
  },
  "sort": [
    { "created_at": "asc" },
    { "_id": "asc" }  // 防止 created_at 重复时排序不稳定
  ],
  "size": 10,
  "search_after": ["2024-12-31T23:59:59", "u_123456"]  // 上一页最后一条记录的排序字段值
}
~~~
### Cache
- QueryCache
  - ES查询的时候，使用filter查询会使用query cache
  - 如果业务场景中的过滤查询比较多，建议将querycache设置大一些，以提高查询速度
- FieldDataCache
  - 在聚类或排序时，field data cache会使用频繁，可以设置字段数据缓存的大小
  - 在聚类或排序场景较多的情形喜爱很有必要，可通过indices.fielddata.cache.size:30%或具体值10GB来设置
  - 但是如果场景或数据变更比较频繁，设置cache并不是好的做法，因为缓存加载的开销也是特别大的
- ShardRequestCache
  - 查询请求发起后，每个分片会将结果返回给协调节点(Coordinating Node),由协调节点将结果整合
  - 如果有需求，可以设置开启；通过设置index.requests.cache.enable: true来开启
  - 不过，shard request cache只缓存hits.total,aggregations,suggestions类型的数据，并不会缓存hits的内容
  - 也可以通过设置indices.requests.cache.size:1%(默认)来控制缓存空间大小
### 其他优化
- query_string或multi_match的查询字段越多，查询越慢，可以在mapping阶段，利用copy_to属性将多字段的值索引到一个新字段，multi_match时，用新的字段查询
- 日期字段的查询，尤其是用now的查询实际上是不存在缓存的，因此，可以从业务的角度来考虑是否一定要用now，毕竟利用querycache是能够大大提高查询效率的
- 查询结果集的大小不能随机设置成大的离谱的值，如query.setSize不能设置成integer.MAX_VALUE,因为ES内部需要建立一个数据结构来放指定大小的结果集数据
- 避免层级过深的聚合查询，层级过深的aggregation，会导致内存，CPU消耗，建议在服务层通过程序来组装业务，也可以通过pipeline的方式来优化
- 复用预索引数据方式来提高AGG性能
## 数据结构优化
> 基于Elasticsearch的使用场景，文档数据结构尽量和使用场景进行结合，去掉没有及不合理的数据
### 减少不需要字段
- 如果ES用于业务搜索服务，一些不需要用于搜索的字段最好不存到ES中
- 这样即节省空间，同时在相同的数据量下，也能提高搜索性能
- 避免使用动态值作字段，动态递增的mapping，会导致集群崩溃
- 同样，也需要控制字段的数量，业务中不使用的字段，就不要索引
- 控制索引的字段数量、mapping深度、索引字段的类型，对于ES的性能优化是重中之重
- 以下是ES关于字段数、mapping深度的一些默认设置
~~~ sh
index.mapping.nested_objects.limit: 10000
index.mapping.total_fields.limit: 1000
index.mapping.depth.limit: 20
~~~
### Nested VS Parent/Child
- 尽量避免使用nested或parent/child的字段，能不用就不用
- nested query慢，parent/child query更慢，比nested query慢上百倍
- 因此能在mapping设计阶段搞定的（大宽表设计或采用比较smart的数据结构），就不要用父子关系的mapping
- 如果一定要使用nested fields,保证nested fields字段不能过多，目前ES默认限制是50
- 因为针对一个document，每一个nested field，都会生成一个独立的document
- 这将使doc数量剧增，影响查询效率，尤其是JOIN的效率
~~~ sh
index.mapping.nested_fields.limit: 50
~~~
| 对比 | Nested Object      | Parent/Child               |
|----|--------------------|----------------------------|
| 优点 | 文档存储在一起，因此读取性高     | 父子文档可以独立更新，互不影响            |
| 缺点 | 更新父文档或子文档时需要更新整个文档 | 为了维护join关系，需要占用部分内存，读取性能较差 |
| 场景 | 子文档偶尔更新，查询频繁       | 子文档更新频繁                    |

### 选择静态映射
> 选择静态映射，非必要时，静止动态映射
- 尽量避免使用动态映射，这样有可能会导致集群崩溃
- 此外，动态映射有可能会带来不可控制的数据类型，进而有可能导致在查询端出现相关异常，影响业务
- 此外，ES作为搜索引擎时，主要承载query的匹配和排序的功能
- 那数据的存储类型基于这两种功能的用途分为两类，一是需要匹配的字段，用来建立倒排索引对query匹配用
- 另一类字段是用做粗排用到的特征字段，如ctr、点击数、评论数等等
### document模型设计
- 对于MySQL，我们经常有一些复杂的关联查询
- 在ES中复杂的关联查询尽量别用，一旦用了性能一般都不太好
- 最好是先在 应用系统里就完成关联，将关联好的数据直接写入es中
- 搜索的时候，就不需要利用es的搜索语法来完成join之类的关联搜索了
- document模型设计是非常重要的，很多操作，不要在搜索的时候才想去执行各种复杂的乱七八糟的操作
- es能支持的操作就那么多，不要考虑用es做一些它不好操作的事情
- 如果真的有那种操作，尽量在document模型设计的时候，写入的时候就完成
- 另外对于一些太复杂的操作，比如join/nested/parent-child搜索都要尽量避免，性能都很差的
## 集群架构设计
### 节点分离
> 主节点、数据节点和协调节点分离
- ES集群在架构拓朴时，采用主节点、数据节点和负载均衡节点分离的架构
- 在5.x版本以后，又可将数据节点再细分为'Hot-Warm'的架构模式
- 主(master)节点
  - 配置node.master:true和node.data:false,该node服务器只作为一个主节点，但不存储任何索引数据
  - 我们推荐每个集群运行3个专用的master节点来提供最好的弹性
  - 使用时，你还需要将discovery.zen.minimum_master_nodes setting参数设置为2，以免出现脑裂（split-brain）的情况
  - 用3个专用的master节点，专门负责处理集群的管理以及加强状态的整体稳定性
  - 因为这3个master节点不包含数据也不会实际参与搜索以及索引操作，在JVM上它们不用做相同的事
  - 例如繁重的索引或者耗时，资源耗费很大的搜索
  - 因此不太可能会因为垃圾回收而导致停顿
  - 因此，master节点的CPU，内存以及磁盘配置可以比data节点少很多的
### 集群分片设置
- ES一旦创建好索引后，就无法调整分片的设置
- 而在ES中，一个分片实际上对应一个Lucene索引
- 而Lucene索引的读写会占用很多的系统资源，因此，分片数不能设置过大
- 所以，在创建索引时，合理配置分片数是非常重要的
- 一般来说，我们遵循一些原则
  - 控制每个分片占用的硬盘容量不超过ES的最大JVM的堆空间设置
  - 一般设置不超过32G，参考上面的JVM内存设置原则
  - 因此，如果索引的总容量在500G左右，那分片大小在16个左右即可
  - 即使保证了1个以上的副本，同样有可能会导致数据丢失，集群无法恢复
  - 所以，一般都设置分配数不超过节点数的3倍

